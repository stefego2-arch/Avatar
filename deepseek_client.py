#!/usr/bin/env python3
"""
ğŸ¤– DEEPSEEK CLIENT â€” Ollama local
Client complet pentru DeepSeek prin Ollama.
Apeluri MINIME È™i inteligente â€” nu la fiecare eveniment.

Import Ã®n alte scripturi:
    from deepseek_client import DeepSeekClient

NecesitÄƒ: pip install requests
          Ollama rulÃ¢nd: ollama serve
          Model: ollama pull deepseek-r1:7b
"""

import json
import re
import time
import threading
from typing import Optional, Callable, Generator

# EliminÄƒ blocurile <think>...</think> pe care deepseek-r1 le adaugÄƒ uneori
_THINK_RE = re.compile(r"<think>.*?</think>", re.DOTALL | re.IGNORECASE)


# â”€â”€â”€ Client principal â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class DeepSeekClient:
    """
    Client pentru DeepSeek via Ollama local.

    Utilizare:
        ds = DeepSeekClient()
        if ds.available:
            raspuns = ds.ask("ExplicÄƒ adunarea pentru un copil de 7 ani.")

        # Streaming (afiÈ™eazÄƒ pe mÄƒsurÄƒ ce vine)
        ds.ask_stream("GenereazÄƒ 3 exerciÈ›ii...", callback=print)

        # VerificÄƒ rÄƒspuns elev
        ok, feedback = ds.check_answer(
            enunt="3 + 4 = ?",
            raspuns_corect="7",
            raspuns_elev="6"
        )

        # GenereazÄƒ exerciÈ›ii noi (pentru populare DB)
        exercises = ds.generate_exercises(
            lesson_title="Adunarea pÃ¢nÄƒ la 10",
            grade=1, subject="MatematicÄƒ", count=5
        )
    """

    MODEL = "deepseek-r1:7b"   # AjusteazÄƒ dacÄƒ ai alt model
    OLLAMA_URL = "http://localhost:11434"
    TIMEOUT_QUICK = 15  # Secunde pentru rÄƒspuns scurt
    TIMEOUT_LONG  = 60  # Secunde pentru generare exerciÈ›ii

    def __init__(self, model: str = None, url: str = None):
        self.model = model or self.MODEL
        self.url = (url or self.OLLAMA_URL).rstrip("/")
        self._available = None  # Lazy check
        self._call_count = 0
        self._total_tokens = 0

        # Circuit breaker: pauzÄƒ dupÄƒ timeout-uri consecutive
        self._consecutive_timeouts = 0
        self._cooldown_until: float = 0.0   # timestamp pÃ¢nÄƒ cÃ¢nd nu mai Ã®ncercÄƒm

        # Cache simplu Ã®n memorie
        self._cache: dict[str, str] = {}
        self.USE_CACHE = True

    # â”€â”€ Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    @staticmethod
    def _strip_think(text: str) -> str:
        """EliminÄƒ blocurile <think>...</think> din rÄƒspunsurile deepseek-r1."""
        return _THINK_RE.sub("", text).strip()

    # â”€â”€ Disponibilitate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    @property
    def available(self) -> bool:
        """VerificÄƒ dacÄƒ Ollama ruleazÄƒ È™i modelul e disponibil."""
        # Circuit breaker: Ã®n cooldown dupÄƒ timeout-uri repetate
        if self._cooldown_until and time.time() < self._cooldown_until:
            return False
        if self._available is None:
            self._available = self._check_available()
        return self._available

    def _check_available(self) -> bool:
        try:
            import requests
            r = requests.get(f"{self.url}/api/tags", timeout=3)
            if r.status_code != 200:
                print("âš ï¸  DeepSeek: Ollama nu rÄƒspunde")
                return False

            models = [m["name"] for m in r.json().get("models", [])]
            if not any(self.model in m for m in models):
                print(f"âš ï¸  DeepSeek: Modelul '{self.model}' nu e instalat.")
                print(f"    Disponibile: {models}")
                print(f"    RuleazÄƒ: ollama pull {self.model}")
                # ÃncearcÄƒ alt model deepseek
                for m in models:
                    if "deepseek" in m.lower() or "llama" in m.lower():
                        self.model = m
                        print(f"    Folosesc: {self.model}")
                        return True
                return False

            print(f"âœ… DeepSeek: {self.model} disponibil")
            return True

        except Exception as e:
            print(f"âš ï¸  DeepSeek: Ollama nu ruleazÄƒ ({e})")
            print("    PorneÈ™te cu: ollama serve")
            return False

    # â”€â”€ API principal â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def ask(self, prompt: str, system: str = None,
            cache_key: str = None, timeout: int = None,
            _force_json: bool = False) -> Optional[str]:
        """
        Trimite o Ã®ntrebare È™i returneazÄƒ rÄƒspunsul complet.

        Args:
            prompt: Mesajul pentru model
            system: InstrucÈ›iune de sistem (opÈ›ional)
            cache_key: Cheie pentru cache (None = nu cache)
            timeout: Timeout Ã®n secunde

        Returns:
            RÄƒspunsul ca string, sau None la eroare
        """
        if not self.available:
            return None

        # VerificÄƒ cache
        if cache_key and self.USE_CACHE and cache_key in self._cache:
            return self._cache[cache_key]

        try:
            import requests
            payload = {
                "model": self.model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "top_p": 0.9,
                    "num_predict": 1024,
                }
            }
            if system:
                payload["system"] = system
            if _force_json:
                payload["format"] = "json"   # Ollama returneazÄƒ JSON valid garantat

            t_start = time.time()
            r = requests.post(
                f"{self.url}/api/generate",
                json=payload,
                timeout=timeout or self.TIMEOUT_QUICK
            )
            t_end = time.time()

            if r.status_code != 200:
                print(f"âŒ DeepSeek: HTTP {r.status_code}")
                return None

            data = r.json()
            response = self._strip_think(data.get("response", ""))

            # Statistici
            self._call_count += 1
            self._consecutive_timeouts = 0   # reset circuit breaker la succes
            tokens = data.get("eval_count", 0)
            self._total_tokens += tokens
            print(f"ğŸ¤– DeepSeek: {len(response)} chars, {tokens} tokens, {t_end-t_start:.1f}s")

            # SalveazÄƒ Ã®n cache
            if cache_key and self.USE_CACHE:
                self._cache[cache_key] = response

            return response

        except Exception as e:
            print(f"âŒ DeepSeek: Eroare: {e}")
            # Circuit breaker: timeout-uri consecutive â†’ pauzÄƒ 5 minute
            err_str = str(e).lower()
            if "timeout" in err_str or "timed out" in err_str or "read timeout" in err_str:
                self._consecutive_timeouts += 1
                if self._consecutive_timeouts >= 2:
                    self._cooldown_until = time.time() + 300   # 5 min
                    print(f"â¸ï¸  DeepSeek: {self._consecutive_timeouts} timeout-uri consecutive "
                          f"â€” pauzÄƒ 5 minute (Ollama supraÃ®ncÄƒrcat)")
            else:
                self._consecutive_timeouts = 0   # resetÄƒm la alte tipuri de erori
            return None

    def ask_collect(self, prompt: str, system: str = None,
                    cache_key: str = None,
                    _force_json: bool = False) -> Optional[str]:
        """
        Varianta streaming a lui ask(): colecteazÄƒ rÄƒspunsul token cu token.
        Avantaj faÈ›Äƒ de ask(): NU existÄƒ read timeout (timeout=(15, None)).
        Util pentru generÄƒri lungi (deepseek-r1:8b + JSON structurat).

        Returns:
            RÄƒspunsul complet ca string, sau None la eroare.
        """
        if not self.available:
            return None

        if cache_key and self.USE_CACHE and cache_key in self._cache:
            return self._cache[cache_key]

        try:
            import requests, json as _json
            payload = {
                "model": self.model,
                "prompt": prompt,
                "stream": True,
                "options": {
                    "temperature": 0.7,
                    "top_p": 0.9,
                    "num_predict": 1024,
                },
            }
            if system:
                payload["system"] = system
            if _force_json:
                payload["format"] = "json"

            t_start = time.time()
            chunks: list[str] = []

            with requests.post(
                f"{self.url}/api/generate",
                json=payload,
                stream=True,
                timeout=(15, None),   # 15s connect, fÄƒrÄƒ timeout pe read
            ) as r:
                if r.status_code != 200:
                    print(f"âŒ DeepSeek collect: HTTP {r.status_code}")
                    return None
                for line in r.iter_lines():
                    if line:
                        try:
                            data = _json.loads(line)
                            chunks.append(data.get("response", ""))
                            if data.get("done"):
                                break
                        except Exception:
                            pass

            t_end = time.time()
            response = self._strip_think("".join(chunks))

            self._call_count += 1
            self._consecutive_timeouts = 0
            print(f"ğŸ¤– DeepSeek: {len(response)} chars, {t_end - t_start:.1f}s (streaming)")

            if cache_key and self.USE_CACHE and response:
                self._cache[cache_key] = response

            return response or None

        except Exception as e:
            print(f"âŒ DeepSeek: Eroare collect: {e}")
            err_str = str(e).lower()
            if "timeout" in err_str or "connect" in err_str:
                self._consecutive_timeouts += 1
                if self._consecutive_timeouts >= 2:
                    self._cooldown_until = time.time() + 300
                    print(f"â¸ï¸  DeepSeek: {self._consecutive_timeouts} erori â€” pauzÄƒ 5 minute")
            else:
                self._consecutive_timeouts = 0
            return None

    def ask_stream(self, prompt: str, callback: Callable[[str], None],
                   system: str = None, on_done: Callable = None):
        """
        Streaming: apeleazÄƒ callback pentru fiecare chunk de text.
        RuleazÄƒ Ã®n thread separat (non-blocking).

        Args:
            prompt: Mesajul
            callback: fn(chunk: str) â€” apelat per token
            system: System prompt
            on_done: fn() â€” apelat la final
        """
        def _stream_thread():
            try:
                import requests
                payload = {
                    "model": self.model,
                    "prompt": prompt,
                    "stream": True,
                    "options": {"temperature": 0.7, "num_predict": 512}
                }
                if system:
                    payload["system"] = system

                with requests.post(
                    f"{self.url}/api/generate",
                    json=payload,
                    stream=True,
                    timeout=self.TIMEOUT_LONG
                ) as r:
                    for line in r.iter_lines():
                        if line:
                            chunk = json.loads(line)
                            token = chunk.get("response", "")
                            if token:
                                callback(token)
                            if chunk.get("done"):
                                break

            except Exception as e:
                print(f"âŒ DeepSeek stream: {e}")
            finally:
                if on_done:
                    on_done()

        if not self.available:
            if on_done:
                on_done()
            return

        t = threading.Thread(target=_stream_thread, daemon=True)
        t.start()

    # â”€â”€ Metode specializate (utilizate de lesson_engine) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def check_answer(self, enunt: str, raspuns_corect: str,
                     raspuns_elev: str, subject: str = "",
                     grade: int = 1) -> tuple[bool, str]:
        """
        VerificÄƒ rÄƒspunsul elevului cu DeepSeek.
        Folosit DOAR pentru rÄƒspunsuri text liber (nu multiple choice).

        Returns:
            (is_correct: bool, feedback: str)
        """
        # Verificare rapidÄƒ Ã®nainte de LLM
        if raspuns_elev.strip().lower() == raspuns_corect.strip().lower():
            return True, "Corect! Bravo! ğŸŒŸ"

        # Verificare numericÄƒ
        try:
            if float(raspuns_elev.strip()) == float(raspuns_corect.strip()):
                return True, "Corect! ğŸŒŸ"
        except ValueError:
            pass

        if not self.available:
            # Fallback simplu fÄƒrÄƒ LLM
            is_correct = raspuns_elev.strip().lower() == raspuns_corect.strip().lower()
            feedback = "Corect! ğŸ‰" if is_correct else f"Nu e corect. RÄƒspunsul corect: {raspuns_corect}"
            return is_correct, feedback

        system = (
            f"EÈ™ti profesor blÃ¢nd pentru copii de clasa {grade} din RomÃ¢nia.\n"
            "VerificÄƒ dacÄƒ rÄƒspunsul elevului este corect sau echivalent cu cel aÈ™teptat.\n"
            "RÄƒspunde EXACT Ã®n formatul:\n"
            "  CORECT | <feedback scurt, max 1-2 propoziÈ›ii, Ã®ncurajator>\n"
            "sau:\n"
            "  GRESIT | <ce era greÈ™it È™i care este rÄƒspunsul corect>\n"
            "NU adÄƒuga alt text Ã®n afara acestui format."
        )

        prompt = f"""ExerciÈ›iu: {enunt}
RÄƒspuns corect: {raspuns_corect}
RÄƒspuns elev: {raspuns_elev}

Este rÄƒspunsul elevului corect?"""

        response = self.ask(prompt, system=system, timeout=10)

        if not response:
            is_correct = raspuns_elev.strip().lower() == raspuns_corect.strip().lower()
            feedback = "Corect! ğŸ‰" if is_correct else f"RÄƒspunsul corect este: {raspuns_corect}"
            return is_correct, feedback

        # Parsing format: "CORECT | feedback" sau "GRESIT | feedback"
        # Fallback la vechiul format (separare pe linie) dacÄƒ modelul nu respectÄƒ |
        if "|" in response:
            parts = response.split("|", 1)
            verdict  = parts[0].strip().upper()
            feedback = parts[1].strip() if len(parts) > 1 else ""
        else:
            lines    = response.strip().split("\n", 1)
            verdict  = lines[0].upper().strip()
            feedback = lines[1].strip() if len(lines) > 1 else response

        is_correct = "CORECT" in verdict and "GRESIT" not in verdict
        if not feedback:
            feedback = "Corect! ğŸŒŸ" if is_correct else f"RÄƒspunsul corect este: {raspuns_corect}"

        return is_correct, feedback

    # RÄƒspunsuri generice pe care LLM le produce cÃ¢nd nu are context bun
    _GENERIC_INVALID = {
        "da", "nu", "da.", "nu.", "corect", "incorect",
        "raspuns", "rÄƒspuns", "...", "â€”", "-", "?", "!",
        "adevÄƒrat", "fals", "adevarat", "fals.", "true", "false",
    }

    @staticmethod
    def _validate_exercise(ex: dict) -> bool:
        """
        ReturneazÄƒ True dacÄƒ exerciÈ›iul e utilizabil.
        FiltreazÄƒ: enunÈ›uri prea scurte, rÄƒspunsuri generice/invalide,
        rÄƒspunsuri-Ã®ntrebÄƒri (LLM confuz), dificultate out-of-range.
        """
        enunt   = (ex.get("enunt")   or "").strip()
        raspuns = (ex.get("raspuns") or "").strip()

        # EnunÈ› prea scurt â€” nu e o Ã®ntrebare realÄƒ
        if len(enunt) < 15:
            return False

        # RÄƒspuns absent sau prea lung (LLM a pus explicaÈ›ie Ã®n loc de rÄƒspuns)
        if not raspuns or len(raspuns) > 120:
            return False

        # LLM a generat o Ã®ntrebare ca rÄƒspuns
        if raspuns.endswith("?"):
            return False

        # RÄƒspuns generic fÄƒrÄƒ valoare pedagogicÄƒ
        if raspuns.lower() in DeepSeekClient._GENERIC_INVALID:
            return False

        # Dificultate â€” normalizeazÄƒ Ã®n loc sÄƒ respingem
        try:
            d = int(ex.get("dificultate") or 1)
            ex["dificultate"] = max(1, min(3, d))
        except (ValueError, TypeError):
            ex["dificultate"] = 1

        return True

    def generate_exercises(self, lesson_title: str, grade: int,
                           subject: str, theory: str = "",
                           count: int = 5, phase: str = "practice",
                           chunk_context: str = "",
                           streaming: bool = False) -> list[dict]:
        """
        GenereazÄƒ exerciÈ›ii noi pentru o lecÈ›ie.
        RULAT OFFLINE (la setup), nu Ã®n timp real.

        Returns:
            list of dicts cu: enunt, raspuns, hint1, hint2, hint3, explicatie, dificultate
        """
        if not self.available:
            print("âš ï¸  DeepSeek indisponibil, nu pot genera exerciÈ›ii")
            return []

        system = f"""EÈ™ti expert Ã®n pedagogie pentru copii de clasa {grade} din RomÃ¢nia.
Generezi exerciÈ›ii clare, potrivite vÃ¢rstei, Ã®n limba romÃ¢nÄƒ.
RÄ‚SPUNZI DOAR CU JSON VALID, fÄƒrÄƒ explicaÈ›ii sau text suplimentar."""

        difficulty_desc = {
            "pretest": "uÈ™oare (dificultate 1-2), sÄƒ testÄƒm cunoÈ™tinÈ›e anterioare",
            "practice": "progresive (dificultate 1-3), pentru exersare",
            "posttest": "moderate (dificultate 2-3), sÄƒ testÄƒm ce s-a Ã®nvÄƒÈ›at"
        }.get(phase, "moderate")

        # Use chunk_context if provided (actual textbook text), else fall back to theory
        content_for_prompt = (chunk_context or theory or "")[:600]

        prompt = f"""GenereazÄƒ {count} exerciÈ›ii {difficulty_desc} pentru:
- Materie: {subject}
- Clasa: {grade}
- LecÈ›ie: {lesson_title}
- Text lecÈ›ie (foloseÈ™te-l pentru a crea exerciÈ›ii RELEVANTE):
{content_for_prompt if content_for_prompt else "N/A"}

REGULI IMPORTANTE:
1. ExerciÈ›iile trebuie sÄƒ fie ÃNTREBÄ‚RI la care copilul rÄƒspunde (NU soluÈ›ii gata-scrise)
2. RÄƒspunsul trebuie sÄƒ fie scurt (1-5 cuvinte) pentru a putea fi verificat automat
3. BazeazÄƒ exerciÈ›iile pe textul de mai sus, nu inventa conÈ›inut
4. Adaptat vÃ¢rstei clasei {grade} â€” propoziÈ›ii simple, cuvinte cunoscute

Format JSON array:
[
  {{
    "enunt": "Ã®ntrebarea / cerinÈ›a pentru elev",
    "raspuns": "raspunsul corect (scurt, 1-5 cuvinte)",
    "hint1": "indiciu vag",
    "hint2": "indiciu mai clar",
    "hint3": "aproape raspunsul",
    "explicatie": "explicatie daca raspunde gresit",
    "dificultate": 1
  }}
]

IMPORTANT: RÄƒspunde NUMAI cu JSON-ul, fÄƒrÄƒ ```json sau altceva."""

        # Cache key includes content hash so different lessons get different exercises
        import hashlib
        content_hash = hashlib.md5(content_for_prompt.encode()).hexdigest()[:8]
        ck = f"ex_{lesson_title}_{phase}_{count}_{content_hash}"
        if streaming:
            response = self.ask_collect(prompt, system=system, cache_key=ck, _force_json=True)
        else:
            response = self.ask(prompt, system=system,
                               cache_key=ck, timeout=self.TIMEOUT_LONG,
                               _force_json=True)

        if not response:
            return []

        try:
            # CurÄƒÈ›Äƒ rÄƒspunsul dacÄƒ are markdown
            clean = response.strip()
            if clean.startswith("```"):
                clean = clean.split("```")[1]
                if clean.startswith("json"):
                    clean = clean[4:]
            if "```" in clean:
                clean = clean.split("```")[0]

            raw = json.loads(clean.strip())
            if not isinstance(raw, list):
                print(f"âŒ DeepSeek: JSON returnat nu e o listÄƒ")
                return []

            exercises = [e for e in raw if self._validate_exercise(e)]
            skipped = len(raw) - len(exercises)
            if skipped:
                print(f"âš ï¸  DeepSeek: {skipped}/{len(raw)} exerciÈ›ii respinse (rÄƒspunsuri invalide)")
            if len(exercises) < len(raw) * 0.5:
                print(f"âš ï¸  DeepSeek: Mai puÈ›in de 50% din exerciÈ›ii au trecut validarea")
            print(f"âœ… DeepSeek: {len(exercises)} exerciÈ›ii valide generate pentru '{lesson_title}'")
            return exercises

        except json.JSONDecodeError as e:
            print(f"âŒ DeepSeek: JSON invalid: {e}")
            print(f"   RÄƒspuns brut: {response[:200]}...")
            return []

    def explain_for_student(self, concept: str, grade: int,
                            student_question: str = "") -> str:
        """
        GenereazÄƒ o explicaÈ›ie personalizatÄƒ pentru un elev.
        Apelat cÃ¢nd elevul nu Ã®nÈ›elege ceva.
        """
        system = f"""EÈ™ti un profesor prietenos È™i rÄƒbdÄƒtor pentru copii de clasa {grade} din RomÃ¢nia.
Explici conceptele simplu, cu exemple din viaÈ›a de zi cu zi, cu entuziasm.
EÈ™ti scurt (max 3-4 propoziÈ›ii) È™i foloseÈ™ti cuvinte simple.
Uneori foloseÈ™ti emoji pentru a fi mai prietenos."""

        prompt = f"""Conceptul de explicat: {concept}
{"Ãntrebarea elevului: " + student_question if student_question else ""}

ExplicÄƒ acest concept simplu, pentru clasa {grade}."""

        response = self.ask(
            prompt, system=system,
            cache_key=f"explain_{concept[:30]}_{grade}" if not student_question else None,
            timeout=15
        )
        return response or f"Hai sÄƒ mai citim Ã®mpreunÄƒ lecÈ›ia despre {concept}!"

    def get_motivation_message(self, student_name: str, score: float,
                               subject: str, grade: int) -> str:
        """GenereazÄƒ un mesaj motivaÈ›ional personalizat."""
        if not self.available:
            # Mesaje predefinite
            if score >= 80:
                return f"Bravo, {student_name}! Ai trecut testul! ğŸ‰"
            elif score >= 60:
                return f"Bine, {student_name}! PuÈ›in mai mult È™i treci! ğŸ’ª"
            else:
                return f"Nu te descuraja, {student_name}! Hai sÄƒ mai exersÄƒm! ğŸ“š"

        system = "EÈ™ti un mentor pozitiv pentru copii. RÄƒspunzi Ã®n 1-2 propoziÈ›ii, entuziast È™i Ã®ncurajator."

        prompt = f"""Elevul {student_name} (clasa {grade}, {subject}) a obÈ›inut scorul {score:.0f}%.
Scrie un mesaj motivaÈ›ional scurt, personalizat, Ã®n romÃ¢nÄƒ."""

        response = self.ask(prompt, system=system, timeout=8)
        return response or f"Bine, {student_name}! ContinuÄƒ sÄƒ lucrezi! ğŸ’ª"

    def answer_free_question(self, question: str, subject: str,
                             grade: int, lesson_context: str = "") -> str:
        """
        RÄƒspunde la o Ã®ntrebare spontanÄƒ a elevului Ã®n timpul lecÈ›iei.
        """
        system = f"""EÈ™ti un profesor virtual pentru un copil de clasa {grade} din RomÃ¢nia.
EÈ™ti prietenos, rÄƒbdÄƒtor È™i explici simplu.
Contextul lecÈ›iei: {subject} - {lesson_context[:200] if lesson_context else "N/A"}
RÄƒspunde scurt (3-5 propoziÈ›ii max), Ã®n romÃ¢nÄƒ, simplu."""

        response = self.ask(question, system=system, timeout=20)
        return response or "E o Ã®ntrebare bunÄƒ! Hai sÄƒ mai citim lecÈ›ia Ã®mpreunÄƒ È™i sÄƒ gÄƒsim rÄƒspunsul!"

    # â”€â”€ Statistici â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def get_stats(self) -> dict:
        return {
            "calls": self._call_count,
            "tokens": self._total_tokens,
            "cache_hits": len(self._cache),
            "model": self.model,
        }


# â”€â”€â”€ Test standalone â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ¤– TEST DEEPSEEK CLIENT")
    print("=" * 60)

    ds = DeepSeekClient()
    print(f"\nDisponibil: {ds.available}")

    if ds.available:
        print("\n1. Test rÄƒspuns simplu:")
        r = ds.ask("Spune 'FuncÈ›ionez!' Ã®n romÃ¢nÄƒ, doar atÃ¢t.")
        print(f"   RÄƒspuns: {r}")

        print("\n2. Test verificare rÄƒspuns:")
        ok, feedback = ds.check_answer(
            enunt="3 + 4 = ?",
            raspuns_corect="7",
            raspuns_elev="7",
            grade=1
        )
        print(f"   Corect: {ok}")
        print(f"   Feedback: {feedback}")

        print("\n3. Test explicaÈ›ie:")
        expl = ds.explain_for_student("adunarea", grade=1)
        print(f"   ExplicaÈ›ie: {expl[:150]}...")

        print("\n4. Test generare exerciÈ›ii:")
        ex = ds.generate_exercises(
            "Adunarea pÃ¢nÄƒ la 10", grade=1, subject="MatematicÄƒ",
            count=2, phase="practice"
        )
        for i, e in enumerate(ex, 1):
            print(f"   ExerciÈ›iu {i}: {e.get('enunt', 'N/A')}")
            print(f"               RÄƒspuns: {e.get('raspuns', 'N/A')}")

        print(f"\nğŸ“Š Statistici: {ds.get_stats()}")
    else:
        print("\nOllama nu ruleazÄƒ. PorneÈ™te cu: ollama serve")
        print("È˜i instaleazÄƒ modelul: ollama pull deepseek-r1:7b")
